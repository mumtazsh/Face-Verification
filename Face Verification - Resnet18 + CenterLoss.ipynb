{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.Resize(224), T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root='classification_data/train_data/', \n",
    "                                                       transform=transform)\n",
    "val_dataset = torchvision.datasets.ImageFolder(root='classification_data/val_data/', \n",
    "                                                       transform=transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root='classification_data/test_data/', \n",
    "                                                       transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "batch_size = 128\n",
    "num_workers = 8 if cuda else 0\n",
    "pin_memory = True if cuda else False\n",
    "numEpochs = 30\n",
    "#num_feats = 3\n",
    "lr = 0.001 #0.001\n",
    "patience = 5\n",
    "factor = 0.316\n",
    "#learningRate = 1e-2\n",
    "weight_decay = 5e-5\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(val_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Net(nn.Module):\n",
    "    def __init__(self, num_classes, feat_dim):\n",
    "        super(Conv_Net, self).__init__()\n",
    "        self.net = torchvision.models.resnet18(num_classes = num_classes)\n",
    "        self.linear_closs = nn.Linear(self.net.fc.in_features, feat_dim, bias=False)\n",
    "        self.relu_closs = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        newmodel = torch.nn.Sequential(*(list(self.net.children())[:-1]))\n",
    "        feature_output = newmodel(x)\n",
    "        label_output = self.net.fc(feature_output.view(-1,self.net.fc.in_features))\n",
    "        #print(feature_output.size())\n",
    "        #print(label_output.size())\n",
    "        closs_output = self.linear_closs(feature_output.view(-1,self.net.fc.in_features))\n",
    "        closs_output = self.relu_closs(closs_output)\n",
    "        return closs_output, label_output, feature_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv_Net(num_classes = 4000, feat_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, feat_dim, device=torch.device('cpu')):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).to(self.device))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long().to(self.device)\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = []\n",
    "        for i in range(batch_size):\n",
    "            value = distmat[i][mask[i]]\n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
    "            dist.append(value)\n",
    "        dist = torch.cat(dist)\n",
    "        loss = dist.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_closs(model, data_loader, test_loader, ver_loader, task='Classification'):\n",
    "    model.train()\n",
    "    if task == 'Classification':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_label, patience=patience, factor=factor, verbose=True)\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_label, patience=patience, factor=factor, verbose=True, mode='max')\n",
    "    for epoch in tqdm(range(numEpochs)):\n",
    "        avg_loss = 0.0\n",
    "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
    "            feats, labels = feats.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer_label.zero_grad()\n",
    "            optimizer_closs.zero_grad()\n",
    "            \n",
    "            feature, outputs, _ = model(feats)\n",
    "\n",
    "            l_loss = criterion_label(outputs, labels.long())\n",
    "            c_loss = criterion_closs(feature, labels.long())\n",
    "            loss = l_loss + closs_weight * c_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer_label.step()\n",
    "            # by doing so, weight_cent would not impact on the learning of centers\n",
    "            for param in criterion_closs.parameters():\n",
    "                param.grad.data *= (1. / closs_weight)\n",
    "            optimizer_closs.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_num % 50 == 49:\n",
    "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
    "                avg_loss = 0.0    \n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del feats\n",
    "            del labels\n",
    "            del loss\n",
    "        \n",
    "        val_loss, val_acc = test_classify_closs(model, test_loader)\n",
    "        print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.format(val_loss, val_acc))\n",
    "        if task == 'Classification':\n",
    "            scheduler.step(val_loss)\n",
    "            #train_loss, train_acc = test_classify_closs(model, data_loader)\n",
    "            #print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
    "             #     format(train_loss, train_acc, val_loss, val_acc))\n",
    "        else:\n",
    "            roc_score = test_verify_closs(model, ver_loader)\n",
    "            print('Roc score: {:.4f}'.\n",
    "                  format(roc_score))\n",
    "            scheduler.step(roc_score)\n",
    "\n",
    "\n",
    "def test_classify_closs(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        feature, outputs, _ = model(feats)\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        l_loss = criterion_label(outputs, labels.long())\n",
    "        c_loss = criterion_closs(feature, labels.long())\n",
    "        loss = l_loss + closs_weight * c_loss\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total\n",
    "\n",
    "def test_verify_closs(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        similarities = []\n",
    "        true_labels = []\n",
    "        for batch_num, (feats1, feats2, labels) in enumerate(test_loader):\n",
    "            feats1, feats2 = feats1.to(device), feats2.to(device)\n",
    "            #feats1 = feats1.to(device)\n",
    "            _, _, output1 = model(feats1)\n",
    "            _, _, output2 = model(feats2)\n",
    "        \n",
    "            cos = nn.CosineSimilarity()\n",
    "        \n",
    "            sim = cos(output1, output2)\n",
    "            similarities.extend(sim)\n",
    "            true_labels.extend(labels)\n",
    "        \n",
    "            del feats1\n",
    "            del feats2\n",
    "            del labels\n",
    "    \n",
    "        true_labels = np.array(true_labels)\n",
    "        similarities = np.array(similarities)\n",
    "        model.train()\n",
    "        return roc_auc_score(true_labels, similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.file1_list = []\n",
    "        self.file2_list = []\n",
    "        self.target_list = []\n",
    "        infile = open(filename , \"r\" )\n",
    "        \n",
    "        for line in infile :\n",
    "            imfile1, imfile2, match = line.split()\n",
    "            self.file1_list.append(imfile1)\n",
    "            self.file2_list.append(imfile2)\n",
    "            self.target_list.append(int(match))\n",
    "        \n",
    "        infile.close()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file1_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1 = Image.open(self.file1_list[index])\n",
    "        img1 = transform(img1)\n",
    "        img2 = Image.open(self.file2_list[index])\n",
    "        img2 = transform(img2)\n",
    "        label = self.target_list[index]\n",
    "        return img1, img2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_set = VerificationDataset(\"verification_pairs_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver_dataloader = DataLoader(verification_set, batch_size=32, shuffle=False, num_workers=4, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv_Net(\n",
       "  (net): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=4000, bias=True)\n",
       "  )\n",
       "  (linear_closs): Linear(in_features=512, out_features=10, bias=False)\n",
       "  (relu_closs): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closs_weight = 0.01\n",
    "lr_cent = 0.5\n",
    "\n",
    "\n",
    "criterion_label = nn.CrossEntropyLoss()\n",
    "criterion_closs = CenterLoss(num_classes, 10, device)\n",
    "optimizer_label = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tBatch: 50\tAvg-Loss: 8.6159\n",
      "Epoch: 1\tBatch: 100\tAvg-Loss: 8.4080\n",
      "Epoch: 1\tBatch: 150\tAvg-Loss: 8.3769\n",
      "Epoch: 1\tBatch: 200\tAvg-Loss: 8.3701\n",
      "Epoch: 1\tBatch: 250\tAvg-Loss: 8.3614\n",
      "Epoch: 1\tBatch: 300\tAvg-Loss: 8.3511\n",
      "Epoch: 1\tBatch: 350\tAvg-Loss: 8.3398\n",
      "Epoch: 1\tBatch: 400\tAvg-Loss: 8.3219\n",
      "Epoch: 1\tBatch: 450\tAvg-Loss: 8.2837\n",
      "Epoch: 1\tBatch: 500\tAvg-Loss: 8.2644\n",
      "Epoch: 1\tBatch: 550\tAvg-Loss: 8.2010\n",
      "Epoch: 1\tBatch: 600\tAvg-Loss: 8.1372\n",
      "Epoch: 1\tBatch: 650\tAvg-Loss: 8.0878\n",
      "Epoch: 1\tBatch: 700\tAvg-Loss: 8.0252\n",
      "Epoch: 1\tBatch: 750\tAvg-Loss: 7.9273\n",
      "Epoch: 1\tBatch: 800\tAvg-Loss: 7.9187\n",
      "Epoch: 1\tBatch: 850\tAvg-Loss: 7.8474\n",
      "Epoch: 1\tBatch: 900\tAvg-Loss: 7.7885\n",
      "Epoch: 1\tBatch: 950\tAvg-Loss: 7.7262\n",
      "Epoch: 1\tBatch: 1000\tAvg-Loss: 7.6783\n",
      "Epoch: 1\tBatch: 1050\tAvg-Loss: 7.6083\n",
      "Epoch: 1\tBatch: 1100\tAvg-Loss: 7.5411\n",
      "Epoch: 1\tBatch: 1150\tAvg-Loss: 7.4907\n",
      "Epoch: 1\tBatch: 1200\tAvg-Loss: 7.4391\n",
      "Epoch: 1\tBatch: 1250\tAvg-Loss: 7.3184\n",
      "Epoch: 1\tBatch: 1300\tAvg-Loss: 7.2802\n",
      "Epoch: 1\tBatch: 1350\tAvg-Loss: 7.2510\n",
      "Epoch: 1\tBatch: 1400\tAvg-Loss: 7.1740\n",
      "Epoch: 1\tBatch: 1450\tAvg-Loss: 7.1026\n",
      "Epoch: 1\tBatch: 1500\tAvg-Loss: 7.0391\n",
      "Epoch: 1\tBatch: 1550\tAvg-Loss: 6.9710\n",
      "Epoch: 1\tBatch: 1600\tAvg-Loss: 6.9122\n",
      "Epoch: 1\tBatch: 1650\tAvg-Loss: 6.8214\n",
      "Epoch: 1\tBatch: 1700\tAvg-Loss: 6.7845\n",
      "Epoch: 1\tBatch: 1750\tAvg-Loss: 6.6919\n",
      "Epoch: 1\tBatch: 1800\tAvg-Loss: 6.6281\n",
      "Epoch: 1\tBatch: 1850\tAvg-Loss: 6.5593\n",
      "Epoch: 1\tBatch: 1900\tAvg-Loss: 6.4767\n",
      "Epoch: 1\tBatch: 1950\tAvg-Loss: 6.4497\n",
      "Epoch: 1\tBatch: 2000\tAvg-Loss: 6.3904\n",
      "Epoch: 1\tBatch: 2050\tAvg-Loss: 6.2666\n",
      "Epoch: 1\tBatch: 2100\tAvg-Loss: 6.2287\n",
      "Epoch: 1\tBatch: 2150\tAvg-Loss: 6.1764\n",
      "Epoch: 1\tBatch: 2200\tAvg-Loss: 6.1053\n",
      "Epoch: 1\tBatch: 2250\tAvg-Loss: 6.0056\n",
      "Epoch: 1\tBatch: 2300\tAvg-Loss: 6.0056\n",
      "Epoch: 1\tBatch: 2350\tAvg-Loss: 5.9164\n",
      "Epoch: 1\tBatch: 2400\tAvg-Loss: 5.8138\n",
      "Epoch: 1\tBatch: 2450\tAvg-Loss: 5.7781\n",
      "Epoch: 1\tBatch: 2500\tAvg-Loss: 5.7584\n",
      "Epoch: 1\tBatch: 2550\tAvg-Loss: 5.6493\n",
      "Epoch: 1\tBatch: 2600\tAvg-Loss: 5.5992\n",
      "Epoch: 1\tBatch: 2650\tAvg-Loss: 5.5706\n",
      "Epoch: 1\tBatch: 2700\tAvg-Loss: 5.4749\n",
      "Epoch: 1\tBatch: 2750\tAvg-Loss: 5.4448\n",
      "Epoch: 1\tBatch: 2800\tAvg-Loss: 5.3863\n",
      "Epoch: 1\tBatch: 2850\tAvg-Loss: 5.2848\n",
      "Epoch: 1\tBatch: 2900\tAvg-Loss: 5.2552\n",
      "Epoch: 1\tBatch: 2950\tAvg-Loss: 5.1626\n",
      "Val Loss: 5.6879\tVal Accuracy: 0.0609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 1/30 [24:38<11:54:31, 1478.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.8282\n",
      "Epoch: 2\tBatch: 50\tAvg-Loss: 4.9791\n",
      "Epoch: 2\tBatch: 100\tAvg-Loss: 4.9232\n",
      "Epoch: 2\tBatch: 150\tAvg-Loss: 4.8520\n",
      "Epoch: 2\tBatch: 200\tAvg-Loss: 4.8119\n",
      "Epoch: 2\tBatch: 250\tAvg-Loss: 4.7712\n",
      "Epoch: 2\tBatch: 300\tAvg-Loss: 4.7707\n",
      "Epoch: 2\tBatch: 350\tAvg-Loss: 4.7272\n",
      "Epoch: 2\tBatch: 400\tAvg-Loss: 4.6687\n",
      "Epoch: 2\tBatch: 450\tAvg-Loss: 4.6240\n",
      "Epoch: 2\tBatch: 500\tAvg-Loss: 4.5601\n",
      "Epoch: 2\tBatch: 550\tAvg-Loss: 4.5297\n",
      "Epoch: 2\tBatch: 600\tAvg-Loss: 4.4709\n",
      "Epoch: 2\tBatch: 650\tAvg-Loss: 4.4287\n",
      "Epoch: 2\tBatch: 700\tAvg-Loss: 4.4567\n",
      "Epoch: 2\tBatch: 750\tAvg-Loss: 4.4300\n",
      "Epoch: 2\tBatch: 800\tAvg-Loss: 4.3033\n",
      "Epoch: 2\tBatch: 850\tAvg-Loss: 4.3198\n",
      "Epoch: 2\tBatch: 900\tAvg-Loss: 4.2687\n",
      "Epoch: 2\tBatch: 950\tAvg-Loss: 4.2978\n",
      "Epoch: 2\tBatch: 1000\tAvg-Loss: 4.2617\n",
      "Epoch: 2\tBatch: 1050\tAvg-Loss: 4.2246\n",
      "Epoch: 2\tBatch: 1100\tAvg-Loss: 4.1045\n",
      "Epoch: 2\tBatch: 1150\tAvg-Loss: 4.1156\n",
      "Epoch: 2\tBatch: 1200\tAvg-Loss: 4.0127\n",
      "Epoch: 2\tBatch: 1250\tAvg-Loss: 4.1000\n",
      "Epoch: 2\tBatch: 1300\tAvg-Loss: 4.0172\n",
      "Epoch: 2\tBatch: 1350\tAvg-Loss: 4.0129\n",
      "Epoch: 2\tBatch: 1400\tAvg-Loss: 3.9652\n",
      "Epoch: 2\tBatch: 1450\tAvg-Loss: 3.9631\n",
      "Epoch: 2\tBatch: 1500\tAvg-Loss: 3.8744\n",
      "Epoch: 2\tBatch: 1550\tAvg-Loss: 3.8314\n",
      "Epoch: 2\tBatch: 1600\tAvg-Loss: 3.8373\n",
      "Epoch: 2\tBatch: 1650\tAvg-Loss: 3.7576\n",
      "Epoch: 2\tBatch: 1700\tAvg-Loss: 3.8167\n",
      "Epoch: 2\tBatch: 1750\tAvg-Loss: 3.7748\n",
      "Epoch: 2\tBatch: 1800\tAvg-Loss: 3.6797\n",
      "Epoch: 2\tBatch: 1850\tAvg-Loss: 3.7380\n",
      "Epoch: 2\tBatch: 1900\tAvg-Loss: 3.6895\n",
      "Epoch: 2\tBatch: 1950\tAvg-Loss: 3.5977\n",
      "Epoch: 2\tBatch: 2000\tAvg-Loss: 3.5870\n",
      "Epoch: 2\tBatch: 2050\tAvg-Loss: 3.6286\n",
      "Epoch: 2\tBatch: 2100\tAvg-Loss: 3.5549\n",
      "Epoch: 2\tBatch: 2150\tAvg-Loss: 3.6232\n",
      "Epoch: 2\tBatch: 2200\tAvg-Loss: 3.4947\n",
      "Epoch: 2\tBatch: 2250\tAvg-Loss: 3.5091\n",
      "Epoch: 2\tBatch: 2300\tAvg-Loss: 3.5119\n",
      "Epoch: 2\tBatch: 2350\tAvg-Loss: 3.4886\n",
      "Epoch: 2\tBatch: 2400\tAvg-Loss: 3.4517\n",
      "Epoch: 2\tBatch: 2450\tAvg-Loss: 3.4521\n",
      "Epoch: 2\tBatch: 2500\tAvg-Loss: 3.3863\n",
      "Epoch: 2\tBatch: 2550\tAvg-Loss: 3.3753\n",
      "Epoch: 2\tBatch: 2600\tAvg-Loss: 3.3594\n",
      "Epoch: 2\tBatch: 2650\tAvg-Loss: 3.3330\n",
      "Epoch: 2\tBatch: 2700\tAvg-Loss: 3.3179\n",
      "Epoch: 2\tBatch: 2750\tAvg-Loss: 3.3046\n",
      "Epoch: 2\tBatch: 2800\tAvg-Loss: 3.3301\n",
      "Epoch: 2\tBatch: 2850\tAvg-Loss: 3.2595\n",
      "Epoch: 2\tBatch: 2900\tAvg-Loss: 3.2351\n",
      "Epoch: 2\tBatch: 2950\tAvg-Loss: 3.1998\n",
      "Val Loss: 3.6098\tVal Accuracy: 0.2860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 2/30 [49:30<11:31:48, 1482.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.8788\n",
      "Epoch: 3\tBatch: 50\tAvg-Loss: 2.8320\n",
      "Epoch: 3\tBatch: 100\tAvg-Loss: 2.8743\n",
      "Epoch: 3\tBatch: 150\tAvg-Loss: 2.8770\n",
      "Epoch: 3\tBatch: 200\tAvg-Loss: 2.8154\n",
      "Epoch: 3\tBatch: 250\tAvg-Loss: 2.8632\n",
      "Epoch: 3\tBatch: 300\tAvg-Loss: 2.8329\n",
      "Epoch: 3\tBatch: 350\tAvg-Loss: 2.8400\n",
      "Epoch: 3\tBatch: 400\tAvg-Loss: 2.8050\n",
      "Epoch: 3\tBatch: 450\tAvg-Loss: 2.8619\n",
      "Epoch: 3\tBatch: 500\tAvg-Loss: 2.8048\n",
      "Epoch: 3\tBatch: 550\tAvg-Loss: 2.8431\n",
      "Epoch: 3\tBatch: 600\tAvg-Loss: 2.8059\n",
      "Epoch: 3\tBatch: 650\tAvg-Loss: 2.7652\n",
      "Epoch: 3\tBatch: 700\tAvg-Loss: 2.7740\n",
      "Epoch: 3\tBatch: 750\tAvg-Loss: 2.7441\n",
      "Epoch: 3\tBatch: 800\tAvg-Loss: 2.7927\n",
      "Epoch: 3\tBatch: 850\tAvg-Loss: 2.7468\n",
      "Epoch: 3\tBatch: 900\tAvg-Loss: 2.7430\n",
      "Epoch: 3\tBatch: 950\tAvg-Loss: 2.8177\n",
      "Epoch: 3\tBatch: 1000\tAvg-Loss: 2.7002\n",
      "Epoch: 3\tBatch: 1050\tAvg-Loss: 2.6788\n",
      "Epoch: 3\tBatch: 1100\tAvg-Loss: 2.6647\n",
      "Epoch: 3\tBatch: 1150\tAvg-Loss: 2.6805\n",
      "Epoch: 3\tBatch: 1200\tAvg-Loss: 2.6128\n",
      "Epoch: 3\tBatch: 1250\tAvg-Loss: 2.6459\n",
      "Epoch: 3\tBatch: 1300\tAvg-Loss: 2.6967\n",
      "Epoch: 3\tBatch: 1350\tAvg-Loss: 2.6224\n",
      "Epoch: 3\tBatch: 1400\tAvg-Loss: 2.6535\n",
      "Epoch: 3\tBatch: 1450\tAvg-Loss: 2.6017\n",
      "Epoch: 3\tBatch: 1500\tAvg-Loss: 2.5731\n",
      "Epoch: 3\tBatch: 1550\tAvg-Loss: 2.6071\n",
      "Epoch: 3\tBatch: 1600\tAvg-Loss: 2.6107\n",
      "Epoch: 3\tBatch: 1650\tAvg-Loss: 2.5668\n",
      "Epoch: 3\tBatch: 1700\tAvg-Loss: 2.5852\n",
      "Epoch: 3\tBatch: 1750\tAvg-Loss: 2.5269\n",
      "Epoch: 3\tBatch: 1800\tAvg-Loss: 2.4954\n",
      "Epoch: 3\tBatch: 1850\tAvg-Loss: 2.4860\n",
      "Epoch: 3\tBatch: 1900\tAvg-Loss: 2.4909\n",
      "Epoch: 3\tBatch: 1950\tAvg-Loss: 2.5369\n",
      "Epoch: 3\tBatch: 2000\tAvg-Loss: 2.5552\n",
      "Epoch: 3\tBatch: 2050\tAvg-Loss: 2.4897\n",
      "Epoch: 3\tBatch: 2100\tAvg-Loss: 2.4599\n",
      "Epoch: 3\tBatch: 2150\tAvg-Loss: 2.4301\n",
      "Epoch: 3\tBatch: 2200\tAvg-Loss: 2.4344\n",
      "Epoch: 3\tBatch: 2250\tAvg-Loss: 2.4158\n",
      "Epoch: 3\tBatch: 2300\tAvg-Loss: 2.3849\n",
      "Epoch: 3\tBatch: 2350\tAvg-Loss: 2.4017\n",
      "Epoch: 3\tBatch: 2400\tAvg-Loss: 2.3803\n",
      "Epoch: 3\tBatch: 2450\tAvg-Loss: 2.4107\n",
      "Epoch: 3\tBatch: 2500\tAvg-Loss: 2.3301\n",
      "Epoch: 3\tBatch: 2550\tAvg-Loss: 2.3187\n",
      "Epoch: 3\tBatch: 2600\tAvg-Loss: 2.3150\n",
      "Epoch: 3\tBatch: 2650\tAvg-Loss: 2.2953\n",
      "Epoch: 3\tBatch: 2700\tAvg-Loss: 2.3484\n",
      "Epoch: 3\tBatch: 2750\tAvg-Loss: 2.3113\n",
      "Epoch: 3\tBatch: 2800\tAvg-Loss: 2.3281\n",
      "Epoch: 3\tBatch: 2850\tAvg-Loss: 2.2937\n",
      "Epoch: 3\tBatch: 2900\tAvg-Loss: 2.3373\n",
      "Epoch: 3\tBatch: 2950\tAvg-Loss: 2.3281\n",
      "Val Loss: 2.7839\tVal Accuracy: 0.4258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 3/30 [1:14:15<11:07:23, 1483.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.8780\n",
      "Epoch: 4\tBatch: 50\tAvg-Loss: 1.9411\n",
      "Epoch: 4\tBatch: 100\tAvg-Loss: 1.8659\n",
      "Epoch: 4\tBatch: 150\tAvg-Loss: 1.8959\n",
      "Epoch: 4\tBatch: 200\tAvg-Loss: 1.8780\n",
      "Epoch: 4\tBatch: 250\tAvg-Loss: 1.8969\n",
      "Epoch: 4\tBatch: 300\tAvg-Loss: 1.8806\n",
      "Epoch: 4\tBatch: 350\tAvg-Loss: 1.9456\n",
      "Epoch: 4\tBatch: 400\tAvg-Loss: 1.9321\n",
      "Epoch: 4\tBatch: 450\tAvg-Loss: 1.9241\n",
      "Epoch: 4\tBatch: 500\tAvg-Loss: 1.9264\n",
      "Epoch: 4\tBatch: 550\tAvg-Loss: 1.9420\n",
      "Epoch: 4\tBatch: 600\tAvg-Loss: 1.9190\n",
      "Epoch: 4\tBatch: 650\tAvg-Loss: 1.9882\n",
      "Epoch: 4\tBatch: 700\tAvg-Loss: 1.9533\n",
      "Epoch: 4\tBatch: 750\tAvg-Loss: 1.9459\n",
      "Epoch: 4\tBatch: 800\tAvg-Loss: 1.9377\n",
      "Epoch: 4\tBatch: 850\tAvg-Loss: 1.8659\n",
      "Epoch: 4\tBatch: 900\tAvg-Loss: 1.9419\n",
      "Epoch: 4\tBatch: 950\tAvg-Loss: 1.9105\n",
      "Epoch: 4\tBatch: 1000\tAvg-Loss: 1.9404\n",
      "Epoch: 4\tBatch: 1050\tAvg-Loss: 1.9752\n",
      "Epoch: 4\tBatch: 1100\tAvg-Loss: 1.9624\n",
      "Epoch: 4\tBatch: 1150\tAvg-Loss: 1.9045\n",
      "Epoch: 4\tBatch: 1200\tAvg-Loss: 1.8953\n",
      "Epoch: 4\tBatch: 1250\tAvg-Loss: 1.9141\n",
      "Epoch: 4\tBatch: 1300\tAvg-Loss: 1.9057\n",
      "Epoch: 4\tBatch: 1350\tAvg-Loss: 1.9345\n",
      "Epoch: 4\tBatch: 1400\tAvg-Loss: 1.9558\n",
      "Epoch: 4\tBatch: 1450\tAvg-Loss: 1.9203\n",
      "Epoch: 4\tBatch: 1500\tAvg-Loss: 1.9269\n",
      "Epoch: 4\tBatch: 1550\tAvg-Loss: 1.9429\n",
      "Epoch: 4\tBatch: 1600\tAvg-Loss: 1.9317\n",
      "Epoch: 4\tBatch: 1650\tAvg-Loss: 1.9119\n",
      "Epoch: 4\tBatch: 1700\tAvg-Loss: 1.9230\n",
      "Epoch: 4\tBatch: 1750\tAvg-Loss: 1.8713\n",
      "Epoch: 4\tBatch: 1800\tAvg-Loss: 1.8670\n",
      "Epoch: 4\tBatch: 1850\tAvg-Loss: 1.8945\n",
      "Epoch: 4\tBatch: 1900\tAvg-Loss: 1.9259\n",
      "Epoch: 4\tBatch: 1950\tAvg-Loss: 1.8967\n",
      "Epoch: 4\tBatch: 2000\tAvg-Loss: 1.8833\n",
      "Epoch: 4\tBatch: 2050\tAvg-Loss: 1.8977\n",
      "Epoch: 4\tBatch: 2100\tAvg-Loss: 1.8739\n",
      "Epoch: 4\tBatch: 2150\tAvg-Loss: 1.8956\n",
      "Epoch: 4\tBatch: 2200\tAvg-Loss: 1.8585\n",
      "Epoch: 4\tBatch: 2250\tAvg-Loss: 1.8784\n",
      "Epoch: 4\tBatch: 2300\tAvg-Loss: 1.8363\n",
      "Epoch: 4\tBatch: 2350\tAvg-Loss: 1.8480\n",
      "Epoch: 4\tBatch: 2400\tAvg-Loss: 1.8485\n",
      "Epoch: 4\tBatch: 2450\tAvg-Loss: 1.8263\n",
      "Epoch: 4\tBatch: 2500\tAvg-Loss: 1.8336\n",
      "Epoch: 4\tBatch: 2550\tAvg-Loss: 1.8492\n",
      "Epoch: 4\tBatch: 2600\tAvg-Loss: 1.7859\n",
      "Epoch: 4\tBatch: 2650\tAvg-Loss: 1.8249\n",
      "Epoch: 4\tBatch: 2700\tAvg-Loss: 1.7686\n",
      "Epoch: 4\tBatch: 2750\tAvg-Loss: 1.8428\n",
      "Epoch: 4\tBatch: 2800\tAvg-Loss: 1.8228\n",
      "Epoch: 4\tBatch: 2850\tAvg-Loss: 1.8251\n",
      "Epoch: 4\tBatch: 2900\tAvg-Loss: 1.8513\n",
      "Epoch: 4\tBatch: 2950\tAvg-Loss: 1.7853\n",
      "Val Loss: 2.3712\tVal Accuracy: 0.5031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 4/30 [1:38:24<10:38:21, 1473.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.8888\n",
      "Epoch: 5\tBatch: 50\tAvg-Loss: 1.3972\n",
      "Epoch: 5\tBatch: 100\tAvg-Loss: 1.3890\n",
      "Epoch: 5\tBatch: 150\tAvg-Loss: 1.4242\n",
      "Epoch: 5\tBatch: 200\tAvg-Loss: 1.4132\n",
      "Epoch: 5\tBatch: 250\tAvg-Loss: 1.4489\n",
      "Epoch: 5\tBatch: 300\tAvg-Loss: 1.4621\n",
      "Epoch: 5\tBatch: 350\tAvg-Loss: 1.4146\n",
      "Epoch: 5\tBatch: 400\tAvg-Loss: 1.4910\n",
      "Epoch: 5\tBatch: 450\tAvg-Loss: 1.4484\n",
      "Epoch: 5\tBatch: 500\tAvg-Loss: 1.4487\n",
      "Epoch: 5\tBatch: 550\tAvg-Loss: 1.4548\n",
      "Epoch: 5\tBatch: 600\tAvg-Loss: 1.5113\n",
      "Epoch: 5\tBatch: 650\tAvg-Loss: 1.5332\n",
      "Epoch: 5\tBatch: 700\tAvg-Loss: 1.5171\n",
      "Epoch: 5\tBatch: 750\tAvg-Loss: 1.5293\n",
      "Epoch: 5\tBatch: 800\tAvg-Loss: 1.5089\n",
      "Epoch: 5\tBatch: 850\tAvg-Loss: 1.4911\n",
      "Epoch: 5\tBatch: 900\tAvg-Loss: 1.5139\n",
      "Epoch: 5\tBatch: 950\tAvg-Loss: 1.5437\n",
      "Epoch: 5\tBatch: 1000\tAvg-Loss: 1.5431\n",
      "Epoch: 5\tBatch: 1050\tAvg-Loss: 1.5443\n",
      "Epoch: 5\tBatch: 1100\tAvg-Loss: 1.5109\n",
      "Epoch: 5\tBatch: 1150\tAvg-Loss: 1.5496\n",
      "Epoch: 5\tBatch: 1200\tAvg-Loss: 1.5401\n",
      "Epoch: 5\tBatch: 1250\tAvg-Loss: 1.5483\n",
      "Epoch: 5\tBatch: 1300\tAvg-Loss: 1.5741\n",
      "Epoch: 5\tBatch: 1350\tAvg-Loss: 1.5686\n",
      "Epoch: 5\tBatch: 1400\tAvg-Loss: 1.5879\n",
      "Epoch: 5\tBatch: 1450\tAvg-Loss: 1.5060\n",
      "Epoch: 5\tBatch: 1500\tAvg-Loss: 1.5940\n",
      "Epoch: 5\tBatch: 1550\tAvg-Loss: 1.5334\n",
      "Epoch: 5\tBatch: 1600\tAvg-Loss: 1.5546\n",
      "Epoch: 5\tBatch: 1650\tAvg-Loss: 1.5583\n",
      "Epoch: 5\tBatch: 1700\tAvg-Loss: 1.5532\n",
      "Epoch: 5\tBatch: 1750\tAvg-Loss: 1.5241\n",
      "Epoch: 5\tBatch: 1800\tAvg-Loss: 1.5405\n",
      "Epoch: 5\tBatch: 1850\tAvg-Loss: 1.5489\n",
      "Epoch: 5\tBatch: 1900\tAvg-Loss: 1.5170\n",
      "Epoch: 5\tBatch: 1950\tAvg-Loss: 1.5665\n",
      "Epoch: 5\tBatch: 2000\tAvg-Loss: 1.5306\n",
      "Epoch: 5\tBatch: 2050\tAvg-Loss: 1.5522\n",
      "Epoch: 5\tBatch: 2100\tAvg-Loss: 1.5455\n",
      "Epoch: 5\tBatch: 2150\tAvg-Loss: 1.5550\n",
      "Epoch: 5\tBatch: 2200\tAvg-Loss: 1.5349\n",
      "Epoch: 5\tBatch: 2250\tAvg-Loss: 1.5838\n",
      "Epoch: 5\tBatch: 2300\tAvg-Loss: 1.5440\n",
      "Epoch: 5\tBatch: 2350\tAvg-Loss: 1.5738\n",
      "Epoch: 5\tBatch: 2400\tAvg-Loss: 1.5090\n",
      "Epoch: 5\tBatch: 2450\tAvg-Loss: 1.5128\n",
      "Epoch: 5\tBatch: 2500\tAvg-Loss: 1.5640\n",
      "Epoch: 5\tBatch: 2550\tAvg-Loss: 1.5522\n",
      "Epoch: 5\tBatch: 2600\tAvg-Loss: 1.5499\n",
      "Epoch: 5\tBatch: 2650\tAvg-Loss: 1.5438\n",
      "Epoch: 5\tBatch: 2700\tAvg-Loss: 1.4917\n",
      "Epoch: 5\tBatch: 2750\tAvg-Loss: 1.4889\n",
      "Epoch: 5\tBatch: 2800\tAvg-Loss: 1.5445\n",
      "Epoch: 5\tBatch: 2850\tAvg-Loss: 1.5226\n",
      "Epoch: 5\tBatch: 2900\tAvg-Loss: 1.5338\n",
      "Epoch: 5\tBatch: 2950\tAvg-Loss: 1.4915\n",
      "Val Loss: 1.9613\tVal Accuracy: 0.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 5/30 [2:02:27<10:10:02, 1464.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.9097\n",
      "Epoch: 6\tBatch: 50\tAvg-Loss: 1.1298\n",
      "Epoch: 6\tBatch: 100\tAvg-Loss: 1.0911\n",
      "Epoch: 6\tBatch: 150\tAvg-Loss: 1.1054\n",
      "Epoch: 6\tBatch: 200\tAvg-Loss: 1.1280\n",
      "Epoch: 6\tBatch: 250\tAvg-Loss: 1.1348\n",
      "Epoch: 6\tBatch: 300\tAvg-Loss: 1.1976\n",
      "Epoch: 6\tBatch: 350\tAvg-Loss: 1.1395\n",
      "Epoch: 6\tBatch: 400\tAvg-Loss: 1.1905\n",
      "Epoch: 6\tBatch: 450\tAvg-Loss: 1.1838\n",
      "Epoch: 6\tBatch: 500\tAvg-Loss: 1.1618\n",
      "Epoch: 6\tBatch: 550\tAvg-Loss: 1.1717\n",
      "Epoch: 6\tBatch: 600\tAvg-Loss: 1.2440\n",
      "Epoch: 6\tBatch: 650\tAvg-Loss: 1.2697\n",
      "Epoch: 6\tBatch: 700\tAvg-Loss: 1.2289\n",
      "Epoch: 6\tBatch: 750\tAvg-Loss: 1.2650\n",
      "Epoch: 6\tBatch: 800\tAvg-Loss: 1.2302\n",
      "Epoch: 6\tBatch: 850\tAvg-Loss: 1.3027\n",
      "Epoch: 6\tBatch: 900\tAvg-Loss: 1.3061\n",
      "Epoch: 6\tBatch: 950\tAvg-Loss: 1.2760\n",
      "Epoch: 6\tBatch: 1000\tAvg-Loss: 1.2735\n",
      "Epoch: 6\tBatch: 1050\tAvg-Loss: 1.2893\n",
      "Epoch: 6\tBatch: 1100\tAvg-Loss: 1.2579\n",
      "Epoch: 6\tBatch: 1150\tAvg-Loss: 1.3384\n",
      "Epoch: 6\tBatch: 1200\tAvg-Loss: 1.2894\n",
      "Epoch: 6\tBatch: 1250\tAvg-Loss: 1.2627\n",
      "Epoch: 6\tBatch: 1300\tAvg-Loss: 1.3103\n",
      "Epoch: 6\tBatch: 1350\tAvg-Loss: 1.2898\n",
      "Epoch: 6\tBatch: 1400\tAvg-Loss: 1.3485\n",
      "Epoch: 6\tBatch: 1450\tAvg-Loss: 1.2950\n",
      "Epoch: 6\tBatch: 1500\tAvg-Loss: 1.3228\n",
      "Epoch: 6\tBatch: 1550\tAvg-Loss: 1.3619\n",
      "Epoch: 6\tBatch: 1600\tAvg-Loss: 1.3038\n",
      "Epoch: 6\tBatch: 1650\tAvg-Loss: 1.3068\n",
      "Epoch: 6\tBatch: 1700\tAvg-Loss: 1.3764\n",
      "Epoch: 6\tBatch: 1750\tAvg-Loss: 1.2987\n",
      "Epoch: 6\tBatch: 1800\tAvg-Loss: 1.3297\n",
      "Epoch: 6\tBatch: 1850\tAvg-Loss: 1.3598\n",
      "Epoch: 6\tBatch: 1900\tAvg-Loss: 1.3273\n",
      "Epoch: 6\tBatch: 1950\tAvg-Loss: 1.2921\n",
      "Epoch: 6\tBatch: 2000\tAvg-Loss: 1.3203\n",
      "Epoch: 6\tBatch: 2050\tAvg-Loss: 1.3177\n",
      "Epoch: 6\tBatch: 2100\tAvg-Loss: 1.3142\n",
      "Epoch: 6\tBatch: 2150\tAvg-Loss: 1.3271\n",
      "Epoch: 6\tBatch: 2200\tAvg-Loss: 1.3098\n",
      "Epoch: 6\tBatch: 2250\tAvg-Loss: 1.3286\n",
      "Epoch: 6\tBatch: 2300\tAvg-Loss: 1.3295\n",
      "Epoch: 6\tBatch: 2350\tAvg-Loss: 1.3151\n",
      "Epoch: 6\tBatch: 2400\tAvg-Loss: 1.2927\n",
      "Epoch: 6\tBatch: 2450\tAvg-Loss: 1.3709\n",
      "Epoch: 6\tBatch: 2500\tAvg-Loss: 1.3282\n",
      "Epoch: 6\tBatch: 2550\tAvg-Loss: 1.3115\n",
      "Epoch: 6\tBatch: 2600\tAvg-Loss: 1.2918\n",
      "Epoch: 6\tBatch: 2650\tAvg-Loss: 1.3398\n",
      "Epoch: 6\tBatch: 2700\tAvg-Loss: 1.2967\n",
      "Epoch: 6\tBatch: 2750\tAvg-Loss: 1.3324\n",
      "Epoch: 6\tBatch: 2800\tAvg-Loss: 1.3686\n",
      "Epoch: 6\tBatch: 2850\tAvg-Loss: 1.3937\n",
      "Epoch: 6\tBatch: 2900\tAvg-Loss: 1.3292\n",
      "Epoch: 6\tBatch: 2950\tAvg-Loss: 1.3044\n",
      "Val Loss: 1.8933\tVal Accuracy: 0.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 6/30 [2:26:39<9:44:10, 1460.44s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.9106\n",
      "Epoch: 7\tBatch: 50\tAvg-Loss: 0.9580\n",
      "Epoch: 7\tBatch: 100\tAvg-Loss: 0.9305\n",
      "Epoch: 7\tBatch: 150\tAvg-Loss: 0.8673\n",
      "Epoch: 7\tBatch: 200\tAvg-Loss: 0.9307\n",
      "Epoch: 7\tBatch: 250\tAvg-Loss: 0.9695\n",
      "Epoch: 7\tBatch: 300\tAvg-Loss: 0.9600\n",
      "Epoch: 7\tBatch: 350\tAvg-Loss: 0.9474\n",
      "Epoch: 7\tBatch: 400\tAvg-Loss: 1.0063\n",
      "Epoch: 7\tBatch: 450\tAvg-Loss: 1.0086\n",
      "Epoch: 7\tBatch: 500\tAvg-Loss: 1.0041\n",
      "Epoch: 7\tBatch: 550\tAvg-Loss: 0.9968\n",
      "Epoch: 7\tBatch: 600\tAvg-Loss: 1.0497\n",
      "Epoch: 7\tBatch: 650\tAvg-Loss: 1.0513\n",
      "Epoch: 7\tBatch: 700\tAvg-Loss: 1.0735\n",
      "Epoch: 7\tBatch: 750\tAvg-Loss: 1.0591\n",
      "Epoch: 7\tBatch: 800\tAvg-Loss: 1.0695\n",
      "Epoch: 7\tBatch: 850\tAvg-Loss: 1.0584\n",
      "Epoch: 7\tBatch: 900\tAvg-Loss: 1.1164\n",
      "Epoch: 7\tBatch: 950\tAvg-Loss: 1.0878\n",
      "Epoch: 7\tBatch: 1000\tAvg-Loss: 1.0848\n",
      "Epoch: 7\tBatch: 1050\tAvg-Loss: 1.0897\n",
      "Epoch: 7\tBatch: 1100\tAvg-Loss: 1.1152\n",
      "Epoch: 7\tBatch: 1150\tAvg-Loss: 1.1483\n",
      "Epoch: 7\tBatch: 1200\tAvg-Loss: 1.1537\n",
      "Epoch: 7\tBatch: 1250\tAvg-Loss: 1.1128\n",
      "Epoch: 7\tBatch: 1300\tAvg-Loss: 1.1484\n",
      "Epoch: 7\tBatch: 1350\tAvg-Loss: 1.1603\n",
      "Epoch: 7\tBatch: 1400\tAvg-Loss: 1.1494\n",
      "Epoch: 7\tBatch: 1450\tAvg-Loss: 1.1188\n",
      "Epoch: 7\tBatch: 1500\tAvg-Loss: 1.1951\n",
      "Epoch: 7\tBatch: 1550\tAvg-Loss: 1.1452\n",
      "Epoch: 7\tBatch: 1600\tAvg-Loss: 1.1472\n",
      "Epoch: 7\tBatch: 1650\tAvg-Loss: 1.1536\n",
      "Epoch: 7\tBatch: 1700\tAvg-Loss: 1.1631\n",
      "Epoch: 7\tBatch: 1750\tAvg-Loss: 1.1536\n",
      "Epoch: 7\tBatch: 1800\tAvg-Loss: 1.1475\n",
      "Epoch: 7\tBatch: 1850\tAvg-Loss: 1.1734\n",
      "Epoch: 7\tBatch: 1900\tAvg-Loss: 1.1644\n",
      "Epoch: 7\tBatch: 1950\tAvg-Loss: 1.1720\n",
      "Epoch: 7\tBatch: 2000\tAvg-Loss: 1.1790\n",
      "Epoch: 7\tBatch: 2050\tAvg-Loss: 1.1730\n",
      "Epoch: 7\tBatch: 2100\tAvg-Loss: 1.1585\n",
      "Epoch: 7\tBatch: 2150\tAvg-Loss: 1.1459\n",
      "Epoch: 7\tBatch: 2200\tAvg-Loss: 1.2225\n",
      "Epoch: 7\tBatch: 2250\tAvg-Loss: 1.1950\n",
      "Epoch: 7\tBatch: 2300\tAvg-Loss: 1.1886\n",
      "Epoch: 7\tBatch: 2350\tAvg-Loss: 1.1555\n",
      "Epoch: 7\tBatch: 2400\tAvg-Loss: 1.1859\n",
      "Epoch: 7\tBatch: 2450\tAvg-Loss: 1.1537\n",
      "Epoch: 7\tBatch: 2500\tAvg-Loss: 1.1405\n",
      "Epoch: 7\tBatch: 2550\tAvg-Loss: 1.1351\n",
      "Epoch: 7\tBatch: 2600\tAvg-Loss: 1.1406\n",
      "Epoch: 7\tBatch: 2650\tAvg-Loss: 1.2173\n",
      "Epoch: 7\tBatch: 2700\tAvg-Loss: 1.2514\n",
      "Epoch: 7\tBatch: 2750\tAvg-Loss: 1.1698\n",
      "Epoch: 7\tBatch: 2800\tAvg-Loss: 1.1974\n",
      "Epoch: 7\tBatch: 2850\tAvg-Loss: 1.1955\n",
      "Epoch: 7\tBatch: 2900\tAvg-Loss: 1.2140\n",
      "Epoch: 7\tBatch: 2950\tAvg-Loss: 1.2273\n",
      "Val Loss: 1.7806\tVal Accuracy: 0.6221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 23%|██▎       | 7/30 [2:50:51<9:18:48, 1457.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.9122\n",
      "Epoch: 8\tBatch: 50\tAvg-Loss: 0.8104\n",
      "Epoch: 8\tBatch: 100\tAvg-Loss: 0.7550\n",
      "Epoch: 8\tBatch: 150\tAvg-Loss: 0.7933\n",
      "Epoch: 8\tBatch: 200\tAvg-Loss: 0.7898\n",
      "Epoch: 8\tBatch: 250\tAvg-Loss: 0.7995\n",
      "Epoch: 8\tBatch: 300\tAvg-Loss: 0.8289\n",
      "Epoch: 8\tBatch: 350\tAvg-Loss: 0.8415\n",
      "Epoch: 8\tBatch: 400\tAvg-Loss: 0.8662\n",
      "Epoch: 8\tBatch: 450\tAvg-Loss: 0.8494\n",
      "Epoch: 8\tBatch: 500\tAvg-Loss: 0.8602\n",
      "Epoch: 8\tBatch: 550\tAvg-Loss: 0.8905\n",
      "Epoch: 8\tBatch: 600\tAvg-Loss: 0.8963\n",
      "Epoch: 8\tBatch: 650\tAvg-Loss: 0.8592\n",
      "Epoch: 8\tBatch: 700\tAvg-Loss: 0.8989\n",
      "Epoch: 8\tBatch: 750\tAvg-Loss: 0.9103\n",
      "Epoch: 8\tBatch: 800\tAvg-Loss: 0.9588\n",
      "Epoch: 8\tBatch: 850\tAvg-Loss: 0.9329\n",
      "Epoch: 8\tBatch: 900\tAvg-Loss: 0.9378\n",
      "Epoch: 8\tBatch: 950\tAvg-Loss: 0.9537\n",
      "Epoch: 8\tBatch: 1000\tAvg-Loss: 0.9594\n",
      "Epoch: 8\tBatch: 1050\tAvg-Loss: 0.9899\n",
      "Epoch: 8\tBatch: 1100\tAvg-Loss: 1.0006\n",
      "Epoch: 8\tBatch: 1150\tAvg-Loss: 0.9782\n",
      "Epoch: 8\tBatch: 1200\tAvg-Loss: 0.9950\n",
      "Epoch: 8\tBatch: 1250\tAvg-Loss: 1.0059\n",
      "Epoch: 8\tBatch: 1300\tAvg-Loss: 0.9700\n",
      "Epoch: 8\tBatch: 1350\tAvg-Loss: 1.0078\n",
      "Epoch: 8\tBatch: 1400\tAvg-Loss: 1.0237\n",
      "Epoch: 8\tBatch: 1450\tAvg-Loss: 1.0335\n",
      "Epoch: 8\tBatch: 1500\tAvg-Loss: 1.0213\n",
      "Epoch: 8\tBatch: 1550\tAvg-Loss: 1.0186\n",
      "Epoch: 8\tBatch: 1600\tAvg-Loss: 1.0357\n",
      "Epoch: 8\tBatch: 1650\tAvg-Loss: 1.0287\n",
      "Epoch: 8\tBatch: 1700\tAvg-Loss: 1.0347\n",
      "Epoch: 8\tBatch: 1750\tAvg-Loss: 1.0730\n",
      "Epoch: 8\tBatch: 1800\tAvg-Loss: 1.0584\n",
      "Epoch: 8\tBatch: 1850\tAvg-Loss: 1.0179\n",
      "Epoch: 8\tBatch: 1900\tAvg-Loss: 1.0261\n",
      "Epoch: 8\tBatch: 1950\tAvg-Loss: 1.0478\n",
      "Epoch: 8\tBatch: 2000\tAvg-Loss: 1.0756\n",
      "Epoch: 8\tBatch: 2050\tAvg-Loss: 1.0216\n",
      "Epoch: 8\tBatch: 2100\tAvg-Loss: 1.0866\n",
      "Epoch: 8\tBatch: 2150\tAvg-Loss: 1.0525\n",
      "Epoch: 8\tBatch: 2200\tAvg-Loss: 1.0079\n",
      "Epoch: 8\tBatch: 2250\tAvg-Loss: 1.0662\n",
      "Epoch: 8\tBatch: 2300\tAvg-Loss: 1.0675\n",
      "Epoch: 8\tBatch: 2350\tAvg-Loss: 1.0673\n",
      "Epoch: 8\tBatch: 2400\tAvg-Loss: 1.1183\n",
      "Epoch: 8\tBatch: 2450\tAvg-Loss: 1.0786\n",
      "Epoch: 8\tBatch: 2500\tAvg-Loss: 1.0690\n",
      "Epoch: 8\tBatch: 2550\tAvg-Loss: 1.0778\n",
      "Epoch: 8\tBatch: 2600\tAvg-Loss: 1.0956\n",
      "Epoch: 8\tBatch: 2650\tAvg-Loss: 1.0207\n",
      "Epoch: 8\tBatch: 2700\tAvg-Loss: 1.0813\n",
      "Epoch: 8\tBatch: 2750\tAvg-Loss: 1.0742\n",
      "Epoch: 8\tBatch: 2800\tAvg-Loss: 1.0946\n",
      "Epoch: 8\tBatch: 2850\tAvg-Loss: 1.1138\n",
      "Epoch: 8\tBatch: 2900\tAvg-Loss: 1.1247\n",
      "Epoch: 8\tBatch: 2950\tAvg-Loss: 1.1364\n",
      "Val Loss: 1.6987\tVal Accuracy: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 8/30 [3:15:00<8:53:35, 1455.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.9038\n",
      "Epoch: 9\tBatch: 50\tAvg-Loss: 0.6979\n",
      "Epoch: 9\tBatch: 100\tAvg-Loss: 0.6628\n",
      "Epoch: 9\tBatch: 150\tAvg-Loss: 0.6751\n",
      "Epoch: 9\tBatch: 200\tAvg-Loss: 0.7163\n",
      "Epoch: 9\tBatch: 250\tAvg-Loss: 0.7034\n",
      "Epoch: 9\tBatch: 300\tAvg-Loss: 0.7056\n",
      "Epoch: 9\tBatch: 350\tAvg-Loss: 0.7382\n",
      "Epoch: 9\tBatch: 400\tAvg-Loss: 0.7270\n",
      "Epoch: 9\tBatch: 450\tAvg-Loss: 0.7736\n",
      "Epoch: 9\tBatch: 500\tAvg-Loss: 0.7784\n",
      "Epoch: 9\tBatch: 550\tAvg-Loss: 0.7973\n",
      "Epoch: 9\tBatch: 600\tAvg-Loss: 0.7701\n",
      "Epoch: 9\tBatch: 650\tAvg-Loss: 0.8051\n",
      "Epoch: 9\tBatch: 700\tAvg-Loss: 0.7762\n",
      "Epoch: 9\tBatch: 750\tAvg-Loss: 0.7898\n",
      "Epoch: 9\tBatch: 800\tAvg-Loss: 0.8467\n",
      "Epoch: 9\tBatch: 850\tAvg-Loss: 0.8807\n",
      "Epoch: 9\tBatch: 900\tAvg-Loss: 0.8500\n",
      "Epoch: 9\tBatch: 950\tAvg-Loss: 0.8616\n",
      "Epoch: 9\tBatch: 1000\tAvg-Loss: 0.8850\n",
      "Epoch: 9\tBatch: 1050\tAvg-Loss: 0.8885\n",
      "Epoch: 9\tBatch: 1100\tAvg-Loss: 0.9061\n",
      "Epoch: 9\tBatch: 1150\tAvg-Loss: 0.9373\n",
      "Epoch: 9\tBatch: 1200\tAvg-Loss: 0.8947\n",
      "Epoch: 9\tBatch: 1250\tAvg-Loss: 0.9104\n",
      "Epoch: 9\tBatch: 1300\tAvg-Loss: 0.9429\n",
      "Epoch: 9\tBatch: 1350\tAvg-Loss: 0.9337\n",
      "Epoch: 9\tBatch: 1400\tAvg-Loss: 0.8831\n",
      "Epoch: 9\tBatch: 1450\tAvg-Loss: 0.9248\n",
      "Epoch: 9\tBatch: 1500\tAvg-Loss: 0.9495\n",
      "Epoch: 9\tBatch: 1550\tAvg-Loss: 0.9093\n",
      "Epoch: 9\tBatch: 1600\tAvg-Loss: 0.9160\n",
      "Epoch: 9\tBatch: 1650\tAvg-Loss: 0.9214\n",
      "Epoch: 9\tBatch: 1700\tAvg-Loss: 0.9511\n",
      "Epoch: 9\tBatch: 1750\tAvg-Loss: 0.9399\n",
      "Epoch: 9\tBatch: 1800\tAvg-Loss: 0.9704\n",
      "Epoch: 9\tBatch: 1850\tAvg-Loss: 0.9436\n",
      "Epoch: 9\tBatch: 1900\tAvg-Loss: 0.9483\n",
      "Epoch: 9\tBatch: 1950\tAvg-Loss: 0.9455\n",
      "Epoch: 9\tBatch: 2000\tAvg-Loss: 0.9685\n",
      "Epoch: 9\tBatch: 2050\tAvg-Loss: 0.9657\n",
      "Epoch: 9\tBatch: 2100\tAvg-Loss: 0.9551\n",
      "Epoch: 9\tBatch: 2150\tAvg-Loss: 0.9942\n",
      "Epoch: 9\tBatch: 2200\tAvg-Loss: 0.9736\n",
      "Epoch: 9\tBatch: 2250\tAvg-Loss: 0.9419\n",
      "Epoch: 9\tBatch: 2300\tAvg-Loss: 0.9727\n",
      "Epoch: 9\tBatch: 2350\tAvg-Loss: 0.9676\n",
      "Epoch: 9\tBatch: 2400\tAvg-Loss: 0.9813\n",
      "Epoch: 9\tBatch: 2450\tAvg-Loss: 0.9498\n",
      "Epoch: 9\tBatch: 2500\tAvg-Loss: 0.9707\n",
      "Epoch: 9\tBatch: 2550\tAvg-Loss: 1.0042\n",
      "Epoch: 9\tBatch: 2600\tAvg-Loss: 0.9783\n",
      "Epoch: 9\tBatch: 2650\tAvg-Loss: 0.9890\n",
      "Epoch: 9\tBatch: 2700\tAvg-Loss: 0.9897\n",
      "Epoch: 9\tBatch: 2750\tAvg-Loss: 1.0218\n",
      "Epoch: 9\tBatch: 2800\tAvg-Loss: 0.9850\n",
      "Epoch: 9\tBatch: 2850\tAvg-Loss: 1.0104\n",
      "Epoch: 9\tBatch: 2900\tAvg-Loss: 1.0038\n",
      "Epoch: 9\tBatch: 2950\tAvg-Loss: 1.0267\n",
      "Val Loss: 1.6713\tVal Accuracy: 0.6492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 9/30 [3:39:04<8:28:10, 1451.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.9203\n",
      "Epoch: 10\tBatch: 50\tAvg-Loss: 0.6712\n",
      "Epoch: 10\tBatch: 100\tAvg-Loss: 0.5937\n",
      "Epoch: 10\tBatch: 150\tAvg-Loss: 0.5493\n",
      "Epoch: 10\tBatch: 200\tAvg-Loss: 0.6143\n",
      "Epoch: 10\tBatch: 250\tAvg-Loss: 0.6273\n",
      "Epoch: 10\tBatch: 300\tAvg-Loss: 0.6441\n",
      "Epoch: 10\tBatch: 350\tAvg-Loss: 0.6437\n",
      "Epoch: 10\tBatch: 400\tAvg-Loss: 0.6707\n",
      "Epoch: 10\tBatch: 450\tAvg-Loss: 0.6504\n",
      "Epoch: 10\tBatch: 500\tAvg-Loss: 0.6629\n",
      "Epoch: 10\tBatch: 550\tAvg-Loss: 0.7089\n",
      "Epoch: 10\tBatch: 600\tAvg-Loss: 0.7287\n",
      "Epoch: 10\tBatch: 650\tAvg-Loss: 0.7607\n",
      "Epoch: 10\tBatch: 700\tAvg-Loss: 0.7471\n",
      "Epoch: 10\tBatch: 750\tAvg-Loss: 0.7760\n",
      "Epoch: 10\tBatch: 800\tAvg-Loss: 0.7790\n",
      "Epoch: 10\tBatch: 850\tAvg-Loss: 0.7753\n",
      "Epoch: 10\tBatch: 900\tAvg-Loss: 0.7517\n",
      "Epoch: 10\tBatch: 950\tAvg-Loss: 0.7519\n",
      "Epoch: 10\tBatch: 1000\tAvg-Loss: 0.7907\n",
      "Epoch: 10\tBatch: 1300\tAvg-Loss: 0.8875\n",
      "Epoch: 10\tBatch: 1350\tAvg-Loss: 0.8440\n",
      "Epoch: 10\tBatch: 1400\tAvg-Loss: 0.8642\n",
      "Epoch: 10\tBatch: 1450\tAvg-Loss: 0.8539\n",
      "Epoch: 10\tBatch: 1500\tAvg-Loss: 0.8565\n",
      "Epoch: 10\tBatch: 1550\tAvg-Loss: 0.8833\n",
      "Epoch: 10\tBatch: 1600\tAvg-Loss: 0.8699\n",
      "Epoch: 10\tBatch: 1650\tAvg-Loss: 0.8712\n",
      "Epoch: 10\tBatch: 1700\tAvg-Loss: 0.8662\n",
      "Epoch: 10\tBatch: 1750\tAvg-Loss: 0.8810\n",
      "Epoch: 10\tBatch: 1800\tAvg-Loss: 0.8492\n",
      "Epoch: 10\tBatch: 1850\tAvg-Loss: 0.8789\n",
      "Epoch: 10\tBatch: 1900\tAvg-Loss: 0.8921\n",
      "Epoch: 10\tBatch: 1950\tAvg-Loss: 0.9015\n",
      "Epoch: 10\tBatch: 2000\tAvg-Loss: 0.8625\n",
      "Epoch: 10\tBatch: 2050\tAvg-Loss: 0.8827\n",
      "Epoch: 10\tBatch: 2100\tAvg-Loss: 0.8614\n",
      "Epoch: 10\tBatch: 2150\tAvg-Loss: 0.8884\n",
      "Epoch: 10\tBatch: 2200\tAvg-Loss: 0.8945\n",
      "Epoch: 10\tBatch: 2250\tAvg-Loss: 0.9018\n",
      "Epoch: 10\tBatch: 2300\tAvg-Loss: 0.9112\n",
      "Epoch: 10\tBatch: 2350\tAvg-Loss: 0.9144\n",
      "Epoch: 10\tBatch: 2400\tAvg-Loss: 0.8969\n",
      "Epoch: 10\tBatch: 2450\tAvg-Loss: 0.9319\n",
      "Epoch: 10\tBatch: 2500\tAvg-Loss: 0.9379\n",
      "Epoch: 10\tBatch: 2550\tAvg-Loss: 0.8955\n",
      "Epoch: 10\tBatch: 2600\tAvg-Loss: 0.9182\n",
      "Epoch: 10\tBatch: 2650\tAvg-Loss: 0.9210\n",
      "Epoch: 10\tBatch: 2700\tAvg-Loss: 0.9286\n",
      "Epoch: 10\tBatch: 2750\tAvg-Loss: 0.9430\n",
      "Epoch: 10\tBatch: 2800\tAvg-Loss: 0.9565\n",
      "Epoch: 10\tBatch: 2850\tAvg-Loss: 0.9250\n",
      "Epoch: 10\tBatch: 2900\tAvg-Loss: 0.9291\n",
      "Epoch: 10\tBatch: 2950\tAvg-Loss: 0.9223\n",
      "Val Loss: 1.6431\tVal Accuracy: 0.6504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 10/30 [4:03:01<8:02:28, 1447.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.9066\n",
      "Epoch: 11\tBatch: 50\tAvg-Loss: 0.5899\n",
      "Epoch: 11\tBatch: 100\tAvg-Loss: 0.5344\n",
      "Epoch: 11\tBatch: 150\tAvg-Loss: 0.5369\n",
      "Epoch: 11\tBatch: 200\tAvg-Loss: 0.5599\n",
      "Epoch: 11\tBatch: 250\tAvg-Loss: 0.5596\n",
      "Epoch: 11\tBatch: 300\tAvg-Loss: 0.5641\n",
      "Epoch: 11\tBatch: 350\tAvg-Loss: 0.5695\n",
      "Epoch: 11\tBatch: 400\tAvg-Loss: 0.5913\n",
      "Epoch: 11\tBatch: 450\tAvg-Loss: 0.6100\n",
      "Epoch: 11\tBatch: 500\tAvg-Loss: 0.6136\n",
      "Epoch: 11\tBatch: 550\tAvg-Loss: 0.6256\n",
      "Epoch: 11\tBatch: 600\tAvg-Loss: 0.6491\n",
      "Epoch: 11\tBatch: 650\tAvg-Loss: 0.6658\n",
      "Epoch: 11\tBatch: 700\tAvg-Loss: 0.7008\n",
      "Epoch: 11\tBatch: 750\tAvg-Loss: 0.6901\n",
      "Epoch: 11\tBatch: 800\tAvg-Loss: 0.6968\n",
      "Epoch: 11\tBatch: 850\tAvg-Loss: 0.7438\n",
      "Epoch: 11\tBatch: 900\tAvg-Loss: 0.7215\n",
      "Epoch: 11\tBatch: 950\tAvg-Loss: 0.7467\n",
      "Epoch: 11\tBatch: 1000\tAvg-Loss: 0.7441\n",
      "Epoch: 11\tBatch: 1050\tAvg-Loss: 0.7551\n",
      "Epoch: 11\tBatch: 1100\tAvg-Loss: 0.7260\n",
      "Epoch: 11\tBatch: 1150\tAvg-Loss: 0.7454\n",
      "Epoch: 11\tBatch: 1200\tAvg-Loss: 0.7946\n",
      "Epoch: 11\tBatch: 1250\tAvg-Loss: 0.7563\n",
      "Epoch: 11\tBatch: 1300\tAvg-Loss: 0.7839\n",
      "Epoch: 11\tBatch: 1350\tAvg-Loss: 0.7815\n",
      "Epoch: 11\tBatch: 1400\tAvg-Loss: 0.7802\n",
      "Epoch: 11\tBatch: 1450\tAvg-Loss: 0.8035\n",
      "Epoch: 11\tBatch: 1500\tAvg-Loss: 0.7575\n",
      "Epoch: 11\tBatch: 1550\tAvg-Loss: 0.8240\n",
      "Epoch: 11\tBatch: 1600\tAvg-Loss: 0.8172\n",
      "Epoch: 11\tBatch: 1650\tAvg-Loss: 0.8107\n",
      "Epoch: 11\tBatch: 1700\tAvg-Loss: 0.8429\n",
      "Epoch: 11\tBatch: 1750\tAvg-Loss: 0.8382\n",
      "Epoch: 11\tBatch: 1800\tAvg-Loss: 0.8418\n",
      "Epoch: 11\tBatch: 1850\tAvg-Loss: 0.7996\n",
      "Epoch: 11\tBatch: 1900\tAvg-Loss: 0.8318\n",
      "Epoch: 11\tBatch: 1950\tAvg-Loss: 0.8362\n",
      "Epoch: 11\tBatch: 2000\tAvg-Loss: 0.8028\n",
      "Epoch: 11\tBatch: 2050\tAvg-Loss: 0.8212\n",
      "Epoch: 11\tBatch: 2100\tAvg-Loss: 0.8263\n",
      "Epoch: 11\tBatch: 2150\tAvg-Loss: 0.8645\n",
      "Epoch: 11\tBatch: 2200\tAvg-Loss: 0.8231\n",
      "Epoch: 11\tBatch: 2250\tAvg-Loss: 0.8343\n",
      "Epoch: 11\tBatch: 2300\tAvg-Loss: 0.8309\n",
      "Epoch: 11\tBatch: 2350\tAvg-Loss: 0.9124\n",
      "Epoch: 11\tBatch: 2400\tAvg-Loss: 0.8608\n",
      "Epoch: 11\tBatch: 2450\tAvg-Loss: 0.8758\n",
      "Epoch: 11\tBatch: 2500\tAvg-Loss: 0.8732\n",
      "Epoch: 11\tBatch: 2550\tAvg-Loss: 0.8519\n",
      "Epoch: 11\tBatch: 2600\tAvg-Loss: 0.8691\n",
      "Epoch: 11\tBatch: 2650\tAvg-Loss: 0.8404\n",
      "Epoch: 11\tBatch: 2700\tAvg-Loss: 0.8796\n",
      "Epoch: 11\tBatch: 2750\tAvg-Loss: 0.8981\n",
      "Epoch: 11\tBatch: 2800\tAvg-Loss: 0.9256\n",
      "Epoch: 11\tBatch: 2850\tAvg-Loss: 0.8492\n",
      "Epoch: 11\tBatch: 2900\tAvg-Loss: 0.8614\n",
      "Epoch: 11\tBatch: 2950\tAvg-Loss: 0.9116\n",
      "Val Loss: 1.6317\tVal Accuracy: 0.6590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 11/30 [4:27:03<7:37:48, 1445.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.9183\n",
      "Epoch: 12\tBatch: 50\tAvg-Loss: 0.5438\n",
      "Epoch: 12\tBatch: 100\tAvg-Loss: 0.4784\n",
      "Epoch: 12\tBatch: 150\tAvg-Loss: 0.5064\n",
      "Epoch: 12\tBatch: 200\tAvg-Loss: 0.4948\n",
      "Epoch: 12\tBatch: 250\tAvg-Loss: 0.5121\n",
      "Epoch: 12\tBatch: 300\tAvg-Loss: 0.5235\n",
      "Epoch: 12\tBatch: 350\tAvg-Loss: 0.5332\n",
      "Epoch: 12\tBatch: 400\tAvg-Loss: 0.5511\n",
      "Epoch: 12\tBatch: 450\tAvg-Loss: 0.5357\n",
      "Epoch: 12\tBatch: 500\tAvg-Loss: 0.5676\n",
      "Epoch: 12\tBatch: 550\tAvg-Loss: 0.5975\n",
      "Epoch: 12\tBatch: 600\tAvg-Loss: 0.5891\n",
      "Epoch: 12\tBatch: 650\tAvg-Loss: 0.5642\n",
      "Epoch: 12\tBatch: 700\tAvg-Loss: 0.6341\n",
      "Epoch: 12\tBatch: 750\tAvg-Loss: 0.5955\n",
      "Epoch: 12\tBatch: 800\tAvg-Loss: 0.6549\n",
      "Epoch: 12\tBatch: 850\tAvg-Loss: 0.6276\n",
      "Epoch: 12\tBatch: 900\tAvg-Loss: 0.6502\n",
      "Epoch: 12\tBatch: 950\tAvg-Loss: 0.7094\n",
      "Epoch: 12\tBatch: 1000\tAvg-Loss: 0.6628\n",
      "Epoch: 12\tBatch: 1050\tAvg-Loss: 0.6906\n",
      "Epoch: 12\tBatch: 1100\tAvg-Loss: 0.6994\n",
      "Epoch: 12\tBatch: 1150\tAvg-Loss: 0.7274\n",
      "Epoch: 12\tBatch: 1200\tAvg-Loss: 0.7444\n",
      "Epoch: 12\tBatch: 1250\tAvg-Loss: 0.7411\n",
      "Epoch: 12\tBatch: 1300\tAvg-Loss: 0.7355\n",
      "Epoch: 12\tBatch: 1350\tAvg-Loss: 0.7386\n",
      "Epoch: 12\tBatch: 1400\tAvg-Loss: 0.7803\n",
      "Epoch: 12\tBatch: 1450\tAvg-Loss: 0.7817\n",
      "Epoch: 12\tBatch: 1500\tAvg-Loss: 0.7701\n",
      "Epoch: 12\tBatch: 1550\tAvg-Loss: 0.7756\n",
      "Epoch: 12\tBatch: 1600\tAvg-Loss: 0.7601\n",
      "Epoch: 12\tBatch: 1650\tAvg-Loss: 0.7641\n",
      "Epoch: 12\tBatch: 1700\tAvg-Loss: 0.7118\n",
      "Epoch: 12\tBatch: 1750\tAvg-Loss: 0.7383\n",
      "Epoch: 12\tBatch: 1800\tAvg-Loss: 0.7791\n",
      "Epoch: 12\tBatch: 1850\tAvg-Loss: 0.7899\n",
      "Epoch: 12\tBatch: 1900\tAvg-Loss: 0.8158\n",
      "Epoch: 12\tBatch: 1950\tAvg-Loss: 0.7977\n",
      "Epoch: 12\tBatch: 2000\tAvg-Loss: 0.7641\n",
      "Epoch: 12\tBatch: 2050\tAvg-Loss: 0.7708\n",
      "Epoch: 12\tBatch: 2100\tAvg-Loss: 0.8089\n",
      "Epoch: 12\tBatch: 2150\tAvg-Loss: 0.8056\n",
      "Epoch: 12\tBatch: 2200\tAvg-Loss: 0.8274\n",
      "Epoch: 12\tBatch: 2250\tAvg-Loss: 0.8063\n",
      "Epoch: 12\tBatch: 2300\tAvg-Loss: 0.8472\n",
      "Epoch: 12\tBatch: 2350\tAvg-Loss: 0.7925\n",
      "Epoch: 12\tBatch: 2400\tAvg-Loss: 0.8135\n",
      "Epoch: 12\tBatch: 2450\tAvg-Loss: 0.7830\n",
      "Epoch: 12\tBatch: 2500\tAvg-Loss: 0.8356\n",
      "Epoch: 12\tBatch: 2550\tAvg-Loss: 0.7989\n",
      "Epoch: 12\tBatch: 2600\tAvg-Loss: 0.7940\n",
      "Epoch: 12\tBatch: 2650\tAvg-Loss: 0.7914\n",
      "Epoch: 12\tBatch: 2700\tAvg-Loss: 0.8480\n",
      "Epoch: 12\tBatch: 2750\tAvg-Loss: 0.8337\n",
      "Epoch: 12\tBatch: 2800\tAvg-Loss: 0.8078\n",
      "Epoch: 12\tBatch: 2850\tAvg-Loss: 0.8405\n",
      "Epoch: 12\tBatch: 2900\tAvg-Loss: 0.8766\n",
      "Epoch: 12\tBatch: 2950\tAvg-Loss: 0.8748\n",
      "Val Loss: 1.6383\tVal Accuracy: 0.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 12/30 [4:51:02<7:13:04, 1443.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roc score: 0.9112\n",
      "Epoch: 13\tBatch: 50\tAvg-Loss: 0.5101\n",
      "Epoch: 13\tBatch: 100\tAvg-Loss: 0.4512\n",
      "Epoch: 13\tBatch: 150\tAvg-Loss: 0.4624\n",
      "Epoch: 13\tBatch: 200\tAvg-Loss: 0.4693\n",
      "Epoch: 13\tBatch: 250\tAvg-Loss: 0.4774\n",
      "Epoch: 13\tBatch: 300\tAvg-Loss: 0.4727\n",
      "Epoch: 13\tBatch: 350\tAvg-Loss: 0.5036\n",
      "Epoch: 13\tBatch: 400\tAvg-Loss: 0.5179\n",
      "Epoch: 13\tBatch: 450\tAvg-Loss: 0.5033\n",
      "Epoch: 13\tBatch: 500\tAvg-Loss: 0.5405\n",
      "Epoch: 13\tBatch: 550\tAvg-Loss: 0.5603\n",
      "Epoch: 13\tBatch: 600\tAvg-Loss: 0.5895\n",
      "Epoch: 13\tBatch: 650\tAvg-Loss: 0.5647\n",
      "Epoch: 13\tBatch: 700\tAvg-Loss: 0.6122\n",
      "Epoch: 13\tBatch: 750\tAvg-Loss: 0.6126\n",
      "Epoch: 13\tBatch: 800\tAvg-Loss: 0.6028\n",
      "Epoch: 13\tBatch: 850\tAvg-Loss: 0.6088\n",
      "Epoch: 13\tBatch: 900\tAvg-Loss: 0.6224\n",
      "Epoch: 13\tBatch: 950\tAvg-Loss: 0.6166\n",
      "Epoch: 13\tBatch: 1000\tAvg-Loss: 0.6308\n",
      "Epoch: 13\tBatch: 1050\tAvg-Loss: 0.6648\n",
      "Epoch: 13\tBatch: 1100\tAvg-Loss: 0.6535\n",
      "Epoch: 13\tBatch: 1150\tAvg-Loss: 0.6493\n",
      "Epoch: 13\tBatch: 1200\tAvg-Loss: 0.6679\n",
      "Epoch: 13\tBatch: 1250\tAvg-Loss: 0.6695\n",
      "Epoch: 13\tBatch: 1300\tAvg-Loss: 0.6844\n",
      "Epoch: 13\tBatch: 1350\tAvg-Loss: 0.7252\n",
      "Epoch: 13\tBatch: 1400\tAvg-Loss: 0.7257\n",
      "Epoch: 13\tBatch: 1450\tAvg-Loss: 0.7010\n",
      "Epoch: 13\tBatch: 1500\tAvg-Loss: 0.6897\n",
      "Epoch: 13\tBatch: 1550\tAvg-Loss: 0.7575\n",
      "Epoch: 13\tBatch: 1600\tAvg-Loss: 0.7276\n",
      "Epoch: 13\tBatch: 1650\tAvg-Loss: 0.7206\n",
      "Epoch: 13\tBatch: 1700\tAvg-Loss: 0.7125\n",
      "Epoch: 13\tBatch: 1750\tAvg-Loss: 0.7080\n",
      "Epoch: 13\tBatch: 1800\tAvg-Loss: 0.7543\n",
      "Epoch: 13\tBatch: 1850\tAvg-Loss: 0.7359\n",
      "Epoch: 13\tBatch: 1900\tAvg-Loss: 0.7513\n",
      "Epoch: 13\tBatch: 1950\tAvg-Loss: 0.7877\n",
      "Epoch: 13\tBatch: 2000\tAvg-Loss: 0.7322\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "train_closs(model, train_dataloader, val_dataloader, ver_dataloader, task='Verification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classify(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './18epochswd5e-5-adam-lranneal-resize.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./26epochswd5e-5-adam-lranneal.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_verify(model, ver_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerificationTestDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        self.file1_list = []\n",
    "        self.file2_list = []\n",
    "        \n",
    "        infile = open(filename , \"r\" )\n",
    "        \n",
    "        for line in infile :\n",
    "            imfile1, imfile2 = line.split()\n",
    "            self.file1_list.append(imfile1)\n",
    "            self.file2_list.append(imfile2)\n",
    "            \n",
    "        \n",
    "        infile.close()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file1_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1 = Image.open(self.file1_list[index])\n",
    "        img1 = transform(img1)\n",
    "        img2 = Image.open(self.file2_list[index])\n",
    "        img2 = transform(img2)\n",
    "        \n",
    "        return img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver_test_set = VerificationTestDataset(\"verification_pairs_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertest_dataloader = DataLoader(ver_test_set, batch_size=32, shuffle=False, num_workers=4, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "        newmodel.to(device)\n",
    "        newmodel.eval()\n",
    "        similarities = []\n",
    "        \n",
    "        for batch_num, (feats1, feats2) in enumerate(test_loader):\n",
    "            feats1, feats2 = feats1.to(device), feats2.to(device)\n",
    "            #feats1 = feats1.to(device)\n",
    "            output1 = newmodel(feats1)\n",
    "            output2 = newmodel(feats2)\n",
    "        \n",
    "            cos = nn.CosineSimilarity()\n",
    "        \n",
    "            sim = cos(output1, output2)\n",
    "            sim = sim.cpu()\n",
    "            similarities.extend(sim.numpy())\n",
    "            \n",
    "        \n",
    "            del feats1\n",
    "            del feats2\n",
    "            \n",
    "    \n",
    "        \n",
    "        similarities = np.array(similarities)\n",
    "        \n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sims = test_final(model, vertest_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sims = np.reshape(test_sims, (51835,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'Id':[\"{} {}\".format(a, b) for a, b in zip(ver_test_set.file1_list, ver_test_set.file2_list)],\n",
    "       'Category': test_sims}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"submission.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
